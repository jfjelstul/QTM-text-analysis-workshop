trump <- select(trump, text, created)
# make corpus
corpus <- Corpus(VectorSource(trump$text))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# make lower case
corpus <- tm_map(corpus, tolower)
corpus
# drop stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus
# stem words
corpus <- tm_map(corpus, stemDocument, language = "english")
corpus
# remove spaces
corpus <- tm_map(corpus, stripWhitespace)
corpus
tm_map(corpus, PlainTextDocument)
corpus$`91`
corpus$content
# make plain text
# this step is critical
corpus <- tm_map(corpus, PlainTextDocument)
corpus$content
# read in data
trump <- read.csv("data/trump-data.csv", stringsAsFactors = FALSE)
# choose variables
trump <- select(trump, text, created)
# make corpus
corpus <- Corpus(VectorSource(trump$text))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# don't worry about this warning
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# make lower case
corpus <- tm_map(corpus, tolower)
# drop stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# stem words
corpus <- tm_map(corpus, stemDocument, language = "english")
# remove spaces
corpus <- tm_map(corpus, stripWhitespace)
# make a document term matrix
# this is the imput for most text analysis techniques
dtm <- DocumentTermMatrix(corpus)
# viewing a DTM
dtm_matrix <- as.matrix(dtm)
# see the dimensions
dim(dtm)
# remove sparce terms
dtm <- removeSparseTerms(dtm, 0.97)
# check dimensions again
dim(dtm)
# the transpose is a term document matrix
tdm <- TermDocumentMatrix(corpus)
# clean workspace
rm(list = ls())
# clear console
cat("\014")
# clean workspace
rm(list = ls())
# clear console
cat("\014")
# vector of file names
files <- c("shakespeare.html", "calculus.html", "biology.html")
# read in data
data <- NULL
for(i in 1:length(files)){
# read in data
text <- readLines(files[i])
# parse HTML
text <- htmlParse(text)
# extract paragraphs
text <- xpathSApply(text, "//p", xmlValue)
# document name
document <- str_replace(files[i], ".html", "")
# make a data frame
text <- data.frame(document = document, text = text, stringsAsFactors = FALSE)
# append this new data frame to the bottom of "data"
data <- rbind(data, text)
}
# drop paragraphs with no text
data <- filter(data, text != "")
# make a text corpus
corpus <- Corpus(VectorSource(data$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus
# make a DTM
dtm <- DocumentTermMatrix(corpus)
# convert to a matrix
dtm_matrix <- as.matrix(dtm)
# make a version with only the most common words
# "sparse" is the maximum allowed sparsity for a term to be kept
# sparsity is the percent of documents a term does NOT appear in
# larger maximum allowed sparsity = more terms retained
dtm_small <- removeSparseTerms(dtm, sparse = 0.90)
dtm_small
# make a version with only the most common words
dtm_small_matrix <- as.matrix(dtm_small)
# view sufficiently frequent terms
findFreqTerms(dtm, lowfreq = 10)
findFreqTerms(dtm, lowfreq = 20)
# word frequencies
frequencies <- colSums(dtm_matrix)
head(frequencies, 25)
# make a data frame with frequent words
summary <- data.frame(word = names(frequencies), frequency = frequencies)
row.names(summary) <- NULL
summary <- arrange(summary, desc(frequency))
View(summary)
# associated words
findAssocs(dtm, "shakespeare", corlimit = 0.37)
findAssocs(dtm, "biology", corlimit = 0.4)
findAssocs(dtm, "calculus", corlimit = 0.35)
a <- as.numeric(dtm_matrix[1,])
b <- as.numeric(dtm_matrix[2,])
# function to calculate cosine similarity
cos_sim <- function(a, b) {
# two ways to write the formula
# c <- sum(a * b) / sqrt(sum(a ^ 2) * sum(b ^ 2))
c <- crossprod(a, b) / sqrt(crossprod(a, a) * crossprod(b, b))
# coerce to numeric
c <- as.numeric(c)
# return the result
return(c)
}
# run the function
cos_sim(a, b)
dtm_matrix
View(dtm_matrix)
a <- as.numeric(dtm_matrix[1,])
b <- as.numeric(dtm_matrix[2,])
a
table(a)
table(b)
# make a text corpus
corpus <- Corpus(VectorSource(data$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
# make a DTM
dtm <- DocumentTermMatrix(corpus)
# convert to a matrix
dtm_matrix <- as.matrix(dtm)
dim(dtm_matrix)
a <- as.numeric(dtm_matrix[1,])
b <- as.numeric(dtm_matrix[2,])
table(a)
table(b)
colSums(dtm_matrix)[1:50]
a <- as.numeric(dtm_matrix[,1])
b <- as.numeric(dtm_matrix[,2])
table(a)
table(b)
# function to calculate cosine similarity
cos_sim <- function(a, b) {
# two ways to write the formula
# c <- sum(a * b) / sqrt(sum(a ^ 2) * sum(b ^ 2))
c <- crossprod(a, b) / sqrt(crossprod(a, a) * crossprod(b, b))
# coerce to numeric
c <- as.numeric(c)
# return the result
return(c)
}
# run the function
cos_sim(a, b)
rowSums(dtm_matrix)
a <- as.numeric(dtm_matrix[5,])
b <- as.numeric(dtm_matrix[10,])
# function to calculate cosine similarity
cos_sim <- function(a, b) {
# two ways to write the formula
# c <- sum(a * b) / sqrt(sum(a ^ 2) * sum(b ^ 2))
c <- crossprod(a, b) / sqrt(crossprod(a, a) * crossprod(b, b))
# coerce to numeric
c <- as.numeric(c)
# return the result
return(c)
}
# run the function
cos_sim(a, b)
install.packages("wordcloud")
# load package
library(wordcloud)
# frequencies
frequencies <- colSums(dtm.matrix)
# frequencies
frequencies <- colSums(dtm_matrix)
# frequency data
summary <- data.frame(word = names(frequencies), frequency = frequencies)
# based on minimum frequency
wordcloud(words = summary$word, freq = summary$frequency, min.freq = 15, random.order = FALSE)
# based on maximum terms
wordcloud(words = summary$word, freq = summary$frequency, max.words = 100, random.order = FALSE)
# install package
install.packages("cluster")
# load library
library(cluster)
# calculate the distance between words
distance <- dist(t(dtm_small_matrix), method = "euclidian")
# run the clustering algorithm
clusters <- hclust(distance, method = "ward.D")
# make a dendrogram
plot(clusters)
# add borders around the clusters
rect.hclust(clusters, k = 5, border = "blue")
# the number of clusters is arbitrary
plot(clusters)
rect.hclust(clusters, k = 7, border = "blue")
# calculate the distance between words
distance <- dist(t(dtm.small.matrix), method="euclidian")
# calculate the distance between words
distance <- dist(t(dtm_small_matrix), method = "euclidian")
# run k-means algorithm
kmeans <- kmeans(distance, 5)
# make a data frame
summary <- data.frame(names(kmeans$cluster), kmeans$cluster, stringsAsFactors = FALSE)
names(summary) <- c("word", "cluster")
rownames(summary) <- NULL
summary <- arrange(summary, cluster)
# plot clusters
clusplot(as.matrix(distance), kmeans$cluster, color = TRUE, shade = FALSE, labels = 2, lines = 0)
# load library
library(topicmodels)
# run a 3 topic model
lda <- LDA(dtm.matrix, k = 3, method = "Gibbs", control = list(burnin = 4000, iter = 2000))
# run a 3 topic model
lda <- LDA(dtm_matrix, k = 3, method = "Gibbs", control = list(burnin = 4000, iter = 2000))
rowSums(dtm_matrix)
# remove rows with no content
dtm_matrix <- dtm_matrix[rowSums(dtm_matrix) != 0, ]
rowSums(dtm_matrix)
# run a 3 topic model
lda <- LDA(dtm_matrix, k = 3, method = "Gibbs", control = list(burnin = 4000, iter = 2000))
# extract top 5 terms
topics <- terms(lda, 5)
names <-  terms(lda, 1)
# view topics
topics
# probabilities
probs <- as.data.frame(lda@gamma)
names(probs) <- names
summary <- cbind(data, probs)
# probabilities
probs <- as.data.frame(lda@gamma)
probs
names
names(probs) <- names
data
# which topic is most likely?
for(i in 1:nrow(probs)) {
summary$topic[i] <- str_c(names[which(summary[i, names] == max(summary[i, names]))], collapse = ", ")
}
View(summary)
summary <- cbind(data[-c(1, 2), ], probs)
# probabilities
probs <- as.data.frame(lda@gamma)
names(probs) <- names
length(probs)
probs
View(data)
summary <- cbind(data[data$text != "", ], probs)
data$text[1:2]
summary <- cbind(data[rowSums(dtm_matrix)!=0, ], probs)
rowSums(dtm_matrix)
# clean workspace
rm(list = ls())
# clear console
cat("\014")
# vector of file names
files <- c("shakespeare.html", "calculus.html", "biology.html")
# read in data
data <- NULL
for(i in 1:length(files)){
# read in data
text <- readLines(files[i])
# parse HTML
text <- htmlParse(text)
# extract paragraphs
text <- xpathSApply(text, "//p", xmlValue)
# document name
document <- str_replace(files[i], ".html", "")
# make a data frame
text <- data.frame(document = document, text = text, stringsAsFactors = FALSE)
# append this new data frame to the bottom of "data"
data <- rbind(data, text)
}
# clean text
data$text <- str_squish(data$text)
# drop paragraphs with no text
data <- filter(data, text != "")
# make a text corpus
corpus <- Corpus(VectorSource(data$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
# make a DTM
dtm <- DocumentTermMatrix(corpus)
# convert to a matrix
dtm_matrix <- as.matrix(dtm)
rowSums(dtm_matrix)
# make a version with only the most common words
# "sparse" is the maximum allowed sparsity for a term to be kept
# sparsity is the percent of documents a term does NOT appear in
# larger maximum allowed sparsity = more terms retained
dtm_small <- removeSparseTerms(dtm, sparse = 0.90)
# make a version with only the most common words
dtm_small_matrix <- as.matrix(dtm_small)
a <- as.numeric(dtm_matrix[1,])
b <- as.numeric(dtm_matrix[2,])
# function to calculate cosine similarity
cos_sim <- function(a, b) {
# two ways to write the formula
# c <- sum(a * b) / sqrt(sum(a ^ 2) * sum(b ^ 2))
c <- crossprod(a, b) / sqrt(crossprod(a, a) * crossprod(b, b))
# coerce to numeric
c <- as.numeric(c)
# return the result
return(c)
}
# run the function
cos_sim(a, b)
# load library
library(topicmodels)
# run a 3 topic model
lda <- LDA(dtm_matrix, k = 3, method = "Gibbs", control = list(burnin = 4000, iter = 2000))
# extract top 5 terms
topics <- terms(lda, 5)
names <-  terms(lda, 1)
# view topics
topics
# probabilities
probs <- as.data.frame(lda@gamma)
names(probs) <- names
summary <- cbind(data[rowSums(dtm_matrix) != 0, ], probs)
# which topic is most likely?
for(i in 1:nrow(probs)) {
summary$topic[i] <- str_c(names[which(summary[i, names] == max(summary[i, names]))], collapse = ", ")
}
# is the topic correct?
summary$correct <- summary$document == summary$topic
# percent correct
mean(summary$correct)
# install packages
install.packages("rworldextra")
install.packages("rworldmap")
install.packages("countrycode")
# load libraries
library(countrycode)
library(rworldmap)
library(rworldxtra)
library(ggplot2)
library(dplyr)
# URL address
url <- "https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita"
# download HTML page
webpage <- readLines(url)
# extract table
table <- readHTMLTable(webpage)
summary(table)
table <- table[[4]]
# rename variables
names(table) <- c("rank", "country", "value")
# convert from a factor to a string
table$value <- as.character(table$value)
# the numbers have commas!
# we can use regular expressions to remove them
table$value <- str_replace_all(table$value, ",", "")
# now convert from a string to a number
table$value <- as.numeric(table$value)
# take the log so the color scale has more variation
table$value <- log(table$value)
# convert names to country codes
table$country <- countrycode(table$country, origin = "country.name", destination = "wb")
View(table)
# download HTML page
webpage <- readLines(url)
# extract table
table <- readHTMLTable(webpage)
summary(table)
table <- table[[4]]
# rename variables
names(table) <- c("rank", "country", "value")
# drop first row
table <- table[-1, ]
# convert from a factor to a string
table$value <- as.character(table$value)
# the numbers have commas!
# we can use regular expressions to remove them
table$value <- str_replace_all(table$value, ",", "")
# now convert from a string to a number
table$value <- as.numeric(table$value)
# take the log so the color scale has more variation
table$value <- log(table$value)
# convert names to country codes
table$country <- countrycode(table$country, origin = "country.name", destination = "wb")
# map coordinates
world_map <- fortify(spTransform(getMap(resolution = "low"), CRS("+proj=wintri")))
world_map$order <- 1:nrow(world_map)
world_map$country <- countrycode(world_map$id, origin = "country.name", destination = "wb")
# merge GDPPC data with map coordinates
world_map <- left_join(world_map, table, by = "country")
# with legend
plot <- ggplot() +
geom_map(data = world_map, map = world.map, mapping = aes(long, lat, map_id = id, fill = value), color = "black", size = 0.2) +
scale_fill_gradient(limits = c(min(world.map$value), max(world.map$value)), na.value = "grey80", name = "GDPPC\n") +
coord_equal() +
theme(axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.line.x = element_blank(),
panel.background = element_blank(),
panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
plot.background = element_blank(),
text = element_text(size = 16))
# with legend
plot <- ggplot() +
geom_map(data = world_map, map = world.map, mapping = aes(long, lat, map_id = id, fill = value), color = "black", size = 0.2) +
scale_fill_gradient(limits = c(min(world_map$value), max(world_map$value)), na.value = "grey80", name = "GDPPC\n") +
coord_equal() +
theme(axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.line.x = element_blank(),
panel.background = element_blank(),
panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
plot.background = element_blank(),
text = element_text(size = 16))
# with legend
plot <- ggplot() +
geom_map(data = world_map, map = world_map, mapping = aes(long, lat, map_id = id, fill = value), color = "black", size = 0.2) +
scale_fill_gradient(limits = c(min(world_map$value), max(world_map$value)), na.value = "grey80", name = "GDPPC\n") +
coord_equal() +
theme_void()
# view plot
plot
# load libraries
library(igraph)
library(ggplot2)
# make DTM
dtm_network <- as.matrix(removeSparseTerms(dtm, sparse = 0.96))
# adjacency matrix
adjacency <- t(dtm_network) %*% dtm_network
# set zeroes in diagonal
diag(adjacency) <- 0
# make a network object
network <- graph.adjacency(adjacency, weighted = TRUE, mode = "undirected")
# choose network layout
network_layout <- layout.fruchterman.reingold(network)
# data frame of nodes
nodes <- as.data.frame(network_layout, stringsAsFactors = FALSE)
names(nodes) <- c("x", "y")
# add node names
nodes$node <- V(network)$name
# data frame of edges
edges <- as.data.frame(get.edgelist(network), stringsAsFactors = FALSE)
names(edges) <- c("from", "to")
# add edge weights
edges$weight <- E(network)$weight
# merge data together
edges <- left_join(edges, nodes, by = c("from" = "node"))
edges <- left_join(edges, nodes, by = c("to" = "node"))
names(edges) <- c("from", "to", "weight", "from.x", "from.y", "to.x", "to.y")
# network theme
network_theme <- function(base_size = 12, base_family = "Helvetica"){
require(grid)
theme_bw(base_size = base_size, base_family = base_family) %+replace%
theme(rect = element_blank(),
line = element_blank(),
text= element_blank())
}
# edge color
edges$color <- "gray85"
edges$color[edges$from == "shakespeare" | edges$to == "shakespeare"] <- "#F8766D"
edges$color[edges$from == "calculus" | edges$to == "calculus"] <- "#00BA38"
edges$color[edges$from == "biology" | edges$to == "biology"] <- "#619CFF"
# edge opacity
edges$alpha <- ifelse(edges$from %in% c("calculus", "biology", "shakespeare") | edges$to %in% c("calculus", "biology", "shakespeare"), 0.7, 0.2)
# node color
nodes$color <- "Black"
nodes$color[nodes$node == "shakespeare"] <- "#F8766D"
nodes$color[nodes$node == "calculus"] <- "#00BA38"
nodes$color[nodes$node == "biology"] <- "#619CFF"
# node size
nodes$size <- ifelse(nodes$node %in% c("calculus", "biology", "shakespeare"), 7, 5)
# make an empty plot object
plot <- ggplot() +
geom_segment(data = edges, aes(x = from.x, xend = to.x, y = from.y, yend = to.y), size = 0.5, color = edges$color, alpha = edges$alpha) +
geom_point(data = nodes, aes(x = x, y = y), size = 3, color = nodes$color) +
geom_text(data = nodes, aes(x = x, y = y, label = node), size = nodes$size, vjust = -1, color = nodes$color) +
network_theme()
# view plot
plot
